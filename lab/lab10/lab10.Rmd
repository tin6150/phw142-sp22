---
title: "Lab 10: Proportions and Inference for Regression"
author: 'Tin Ho 30335-18349'
date: "2022-04-17"
output: pdf_document
---


**Run this chunk of code to load the autograder package!**
```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(testthat)
```

### Instructions 
* Due date: Tuesday, April 19th, at 10:00pm PST with 2 hour grace period.
* Late penalty: 50% late penalty if submitted within 24 hours of due date, no 
marks for assignments submitted thereafter.
* This assignment is graded on **correct completion**, all or nothing. You must pass all public tests and submit the assignment for credit.
* Submission process: Follow the submission instructions on the final page. Make sure you do not remove any `\newpage` tags or rename this file, as this will break the submission.


## Boston Data on Median Household Value and Distance to Employment Centers

We are examining a dataset used to predict housing prices in the area around Boston (Harrison, D. and Rubinfeld, 1978). We wish to specifically examine the association of the measure of housing price (`medv`, median value of owner-occupied homes in the $1000s) and a measure of adjacency to employment (a weighted distance, `dis`, in miles). The data frame (Boston) is contained in another package (`MASS`), which we load into the object `boston2` below.

```{r, message=F}
### NOTE: All of the code is to get you started on the lab. You do not need to
### understand any functions below that you have not seen before.

# Load library with data
library(MASS)

### NOTE: This package has a function `select()` that can be confused with
### dplyr's select. To overcome this, we first import the data we need and then
### detach the library before loading dplyr.

# List variables
boston2 <- 
  read.csv("data/Boston.csv")

# Variable definition - take a quick look at the variables in the data frame
# help(Boston)
detach(package:MASS)
library(broom)
library(dplyr)
library(ggplot2)
library(tidyr)
library(patchwork)
library(testthat)

### Normally when we are doing inference, we take a random sample to make an inference about the population. 
### If you have time after the lab, take a random sample of 50 rows from the data and perform the analysis below to see how your results change.
```

\newpage

## Section 1: Inference for Regression

**1. [1 point] Use the `boston2` data to perform a linear regression of `medv` (median value of owner-occupied homes in $1000s) versus `dis` (weighted mean of distances to five Boston employment centers) and tidy the results. Assign the *linear model* to `p1`.** 

```{r}

#str(boston2)

p1 <- NULL # YOUR CODE HERE
linear_model <- lm( medv ~ dis, data=boston2 )
p1 <- linear_model


m <- linear_model$coefficients[[2]] # slope ( [[]] returns a number )
b <- linear_model$coefficients[[1]] # intercept
b <- 10#656

library(ggplot2)
viz <- ggplot(boston2, aes( x=dis, y=medv)) + geom_point() 
viz2 <- viz + geom_abline( intercept=b, slope=m, lty=2, col="green")
#viz2

tidy(p1)

```

```{r}
. = ottr::check("tests/p1.R")
```

\newpage

**2. Interpret the slope parameter and the p-value from the output above. What null and alternative hypotheses does this p-value refer to?** 

_the slope coefficient is 1.0916, which is to say for each 1 unit change in the x axis (weighted mean distance in mile), the corresponding change on the y axis (median value in 1000s) increases by a factor of 10.916.  The null hypothesis is for a slop of 0, a flat line, indicating no correlation between distance and median value of houses.  The low p-value of 1e-8 is smaller than alpha=0.5, thus we have evidence to reject the null hypothesis, and have evidence to support that alternative hypothesis: that there is a correlation between distance and median house value._


\newpage

**3. [1 point] Derive a 95% CI for this slope parameter and assign the lower and upper bounds to `p3`. Round your bounds to 4 decimal places. In your opinion, would you expect the direction of this relationship to hold if the data were collected today?**

```{r}
p3 <- NULL # YOUR CODE HERE
m <- linear_model$coefficients[[2]]  # 1.0916
b.hat <- m
SE <- 0.1883784 # from table tidy(linear_model)
n <- 506
t.star = qt( p=0.975, df=(n-2) )  # 1.964682

ci.lower <- b.hat - t.star * SE
ci.upper <- b.hat + t.star * SE


p3 <- round( c(ci.lower, ci.upper), 4 )
p3

```

```{r}
. = ottr::check("tests/p3.R")
```
_The 95% CI was found to be (0.7215, 1.4617).  I won't quite expect the same pattern to hold today.  Suburbia is becoming out of style, commute time is a nagative factor for much people.  It seems that nowadays, hip cities like Boston and SF, the closer to city center (work), the higher the house price is.  But distance is just too simplistic of a measure to predict house prices, there are many other variables and cofounders._

\newpage

**4. [1 point] Use a function to look at the r-squared value for this model. Assign this value to `p4` and round to two decimal places. Does `dis` explain a lot of the variance in median household value? Would you expect it to?**

```{r}
p4 <- NULL # YOUR CODE HERE
p4 <- round( glance( linear_model )$r.squared, 2 )
p4

```

```{r}
. = ottr::check("tests/p4.R")
```
_No, given the r.squared value of only 0.6, dis does NOT explain well the medv median value of the _

\newpage

**5. [2 points] Make a plot with the raw data points, use `geom_smooth()` to add the line of best fit from the simple linear regression model (containing `medv` and `dis`), and use `geom_abline()` to add a horizontal line with a slope of 0 that crosses the y-axis at the average value of `medv` to vertically bisect the data points. Store your plot as the object `p5`.**

```{r}
p5 <- NULL # YOUR CODE HERE

viz  <- ggplot(boston2, aes( x=dis, y=medv)) + geom_point() 
viz3 <- viz + geom_smooth( method="lm", se=FALSE) + 
  geom_abline( slope=0, intercept=mean(boston2$medv), col="red" )

p5 <-viz3
p5
```

```{r}
. = ottr::check("tests/p5.R")
```

\newpage

**6. Create plots to check the assumptions required for using simple linear models. You will first need to fit the linear model and then use the `augment()` function from the `broom` package to store the residuals and fitted values into a new data frame. Do the plots raise any concerns about the assumptions of the linear regression you just performed?** 

```{r}


# lab10 video @ [8:40]

data_with_predictions <- augment( linear_model )

# first plot
plot1 <- NULL # YOUR CODE HERE
plot1 <- ggplot( data_with_predictions, aes(x=dis, y=medv)) +
  geom_smooth( method="lm", se=F ) + 
  geom_point( alpha=0.5 ) +
  geom_segment( aes( xend=dis, yend=.fitted), lty=2, alpha=0.5 ) +
  theme_minimal( base_size=16) + 
  labs( title="(a) Scatter plot")

plot1

# second plot
plot2 <- NULL # YOUR CODE HERE
plot2 <- ggplot( data_with_predictions, aes( sample=.resid )) +
  geom_qq() + geom_qq_line() + 
  labs( y="residuals", x="Theoretical quantiles", title="(b) QQplot")
plot2

# third plot
plot3 <- NULL # YOUR CODE HERE
plot3 <- ggplot( data_with_predictions, aes( y=.resid, x=.fitted )) +
  geom_point() +
  geom_hline( aes( yintercept=0 )) +
  labs( y="Residuals", x="Fitted values", title="(c) Fitted vs residuals")
plot3
# expect random scatter w/o pattern ==> good fit.  we don't have that here 

# video @ [11:11]
# see https://piazza.com/class/kyf23hxwr4r52s?cid=402
# The parallel code for this question is in the regression inference.Rmd in the lecture folder on DataHub. It's under the "Example 1: Investigating the assumptions" header.


plot4 <- NULL # YOUR CODE HERE

predictions_reshaped <- data_with_predictions %>% select(.resid, .fitted) %>%
  gather(key = "type", value = "value", .fitted, .resid)

plot4 <- ggplot( predictions_reshaped, aes( y=value )) +
  geom_boxplot( aes( fill=type ) ) + 
  labs(title="(d) Amount explained")

plot4
```

_Yes, these plots raise many concerns on this linear model of dis vs medv.  (a) scatter plots shows that many predicted b.hat values are far from the linear model.  (b) qqplot shows that there is a huge deviation at the top right corner, indicating many outliers.  (c) scatter plot should be random, but I think I see a fan in pattern._

Regardless of your answer, we will continue using the model to make inferences about the relationship between `med` and `dis`.


\newpage

### Pointwise Confidence Intervals and Multiple Testing

As you learned in lecture, there are two types of confidence intervals applicable to estimating a point on the plot which are related to whether one is predicting the population average among individuals with $X=x$ (**mean response**) or whether one is predicting the actual $Y$ for a particular individual (**single observation**). For this assignment, we will concentrate on the confidence interval for the mean response. We do so because it is rare to use statistical models in public health as forecasting models (predicting an individual's health in the future) and is more common to use them to estimate population-level changes (how the mean of a health variable changes in a population as we change exposure). However, as precision medicine becomes more of a reality and models more accurately predict health (i.e., have high $R^2$'s), then statistical forecasting may become more common in our field.

**7. [1 point] Calculate four 95% confidence intervals for the mean response, one at each `dis` value: 2.5, 5.0, 7.5, and 10.0 miles. Create a vector of the lower bounds for each confidence interval rounded to two decimal places and assign it to `p12`.**

**Hint: Use the `predict()` function and be sure to specify interval = "confidence"**

OPTIONAL: If time allows, add the four CIs to a scatter plot of the data (along with the line of best fit).

```{r}
ci_dataframe <- data.frame(dis = c(2.5, 5.0, 7.5, 10))
newdata <- ci_dataframe

p7 <- NULL # YOUR CODE HERE
# linear_model is the lm() for boston2 defined above for p1.
pointwise_ci <- predict( linear_model,  newdata, interval="confidence") %>% cbind( newdata )

p7 <- round( 
            pointwise_ci %>%  pull(lwr)
            , 2 )

p7

p7_plot <- ggplot( boston2, aes( x=dis, y=medv)) +
  geom_point() +
  geom_smooth( method="lm", se=F ) + 
  geom_segment( data=pointwise_ci, 
                aes( x=dis, xend=dis, y=lwr, yend=upr), 
                color="red")
# color can be shortened to col, but then that maybe confusing with column
p7_plot

```

```{r}
. = ottr::check("tests/p7.R")
```

\newpage

**8. Interpret the point wise 95% confidence interval of the median house price when the distance = 10.** 

_If we use the same sampling method and take 100 samples of houses in Boston with a distance of 10 miles to a Bsoton employment center, we would expect the CI of 95 of those samples to contain the true neighborhood median home value to be between 26.88 and 31.73 (in thousands of dollars)_

// video @ 13:21

\newpage

**9. Do the CI's differ in length for different values of `dis`? Why or why not?**

_Yes, the CI range becomes wider as dis gets to a larger value, this is because the predictions is further from the model, there are more variability, thus resulting in a less precise CI._

\newpage

## Section 2: Inference for Proportions

**The rest of the lab assignment is for practice only. You are responsible for understanding the material but these questions will not be graded.**



## Tests of Changes in Sex Ratios Based on a Single Sample

There is a long literature studying changes in sex-ratios of births due to stressful events, such as 9/11. In today's lab, we consider a relatively small study that recorded biomarkers of stress on pregnancy. In the group of subjects that had the highest markers of stress (based on cortisol), there were 14 births to males out of a total of 38.

In this lab, we will compare the four methods we learned to calculate CIs for proportions. Recall that two of these methods involved hand calculations (though we can treat R as if it were a calculator) and two of the methods used built-in R functions.

**10. Use the Normal approximation to construct a 95% confidence interval in this high stress group. We also called this specific method of constructing the CI the "large sample method". Assign the object `large_sample_ci` to a vector of the lower bound and upper bound rounded to 4 decimal places.**

```{r large-sample-method}
#### video @ 14:45

large_sample_ci <- NULL # YOUR CODE HERE

n_q10  <- 38
n      <- n_q10
pb <- 14/38 # proportion of boys aka lead :P

#### CI: estimate -/+ criticalVal * variability

p.hat <- pb
z.star <- qnorm(0.025, lower.tail=F)  #1.96
se_q10 <- sqrt( ( p.hat * (1-p.hat) ) / n   )


large_sample_ci <- round( c(
 p.hat - (z.star * se_q10) , 
 p.hat + (z.star * se_q10)  ), 4)

m1 <- c( large_sample_ci, p.hat )

large_sample_ci

```

```{r}
. = ottr::check("tests/p10.R")
```
_The 95% CI was found to have proportion range from 21.51% to 52.18%._

\newpage

**11. Create the 95% CI again, this time using the R function that implements the Wilson Score method with a continuity correction. Round your answer to 4 decimal places.**

```{r Wilson-score-method}
wilson_score_ci <- NULL # YOUR CODE HERE

#### Method 3, prop.test()
method3 <- prop.test(x=14, n=38, conf.level=0.95)
wilson_score_ci <- round( method3$conf.int[1:2], 4 )
m3 <- c( wilson_score_ci, p.hat )
wilson_score_ci
```

```{r}
. = ottr::check("tests/p11.R")
```
_The 95% CI using wilson score was (22.29%,  54.00%)._

\newpage

**12. Create the 95% CI again, this time using the Plus 4 method. Round your answer to 4 decimal places.**

```{r plus-4-method}
plus_4_ci <- NULL # YOUR CODE HERE

p.tilde <- (14+2)/(38+4)
se.til  <- sqrt(  p.tilde * (1-p.tilde)/(38+4) )

ci_low   <- p.tilde - (z.star * se.til)
ci_high  <- p.tilde + (z.star * se.til)



plus_4_ci <- round( c( ci_low, ci_high ), 4)

m2 <- c( plus_4_ci, p.tilde )  # or was i still to use p.hat as the estimate ? ++++

plus_4_ci

```

```{r}
. = ottr::check("tests/p12.R")
```
_Using the Plus4 method, 95% CI was found to be (23.41%, 52.78%)_

\newpage

**13. Create the 95% CI again, this time using the R function that implements the Clopper Pearson (Exact) method. Round your answer to 4 decimal places.**

```{r Clopper-Pearson-method}
exact_method_ci <- NULL # YOUR CODE HERE

meth4 <- binom.test(x=14, n=38, conf.level=0.95)

exact_method_ci <- round( meth4$conf.int[1:2], 4 )

m4 <- c( exact_method_ci, p.hat )
exact_method_ci


```

```{r}
. = ottr::check("tests/p13.R")
```
_The clover Pearson method found the 95% CI to be (21.81%, 54.01%)_

\newpage

**14. Here is a code template to help you to graphically present these estimates. Graphical presentations of estimates and their CIs is very useful for assessing whether the CIs overlap the null hypothesized value and tends to be better than presenting tables of estimates to readers of your research. Fill in the code below with your estimates for each confidence interval method, assign `p14` to the plot of the confidence intervals, then answer the question below.**

```{r}
# First make a tibble (an easy way to make a data frame) with the data about
# each confidence interval. To do this, replace each instance of 0.00 with the
# estimate from your calculations above.
sex_CIs <- tibble(method   = c("large sample", "Exact", "Wilson", "Plus 4"),
                  lower_CI = c(m1[1]       , m4[1]    ,  m3[1]    ,  m2[1]),
                  upper_CI = c(m1[2]       , m4[2]    ,  m3[2]    ,  m2[2]),
                  estimate = c(m1[3]       , m4[3]    ,  m3[3]    ,  m2[3])
                  )

#                  lower_CI = c(0.0           , 0.0    ,  0.0    ,  0.0),
#                  upper_CI = c(0.0           , 0.0    ,  0.0    ,  0.0),
#                  estimate = c(0.0           , 0.0    ,  0.0    ,  0.0)


# video @ 20:20
# Build the ggplot incrementally, to understand how it works.
# Step 1: (why do we put a horizontal line at 50?)
no.show <- ggplot(data = sex_CIs, aes(x = method, y = estimate)) + 
  geom_point() + 
  geom_hline(aes(yintercept = 50), lty = 2)


# Step 2:
p14 <- ggplot(data = sex_CIs, aes(x = method, y = estimate)) + 
  geom_point() +
  geom_hline(aes(yintercept = 0.50), lty = 2) +
  geom_segment(aes(x = method, xend = method, y = lower_CI, yend = upper_CI)) +
  labs(y = "Estimate with 95% CI")


p14



```

```{r}
. = ottr::check("tests/p14.R")
```

What does `geom_segment()` do? In particular, what do `x`, `xend`, `y` and `yend` specify in this case?

_geom_segment draws a line segment.  it should have 4 coordinate points, but our x-axis is a categorical variable, thus both start and ending point for the x coordinate is the "method" variable, and R will draw a line of width assigned to the category.  For the end coordinate of y, it spans from y to yend, which we specify as lower and upper limit of the CI interval_

\newpage

**15. Based on this plot, what can you say about the confidence intervals for the sex ratio in the high stress group?**

_In this high stress group, their mean ratio for their new born is noticeably away from the expected 0.5 50% girls 50% boys ratio.  However, the 95% CI used in all 4 methods include the 0.5 halfway mark, thus there isn't enough evidence to support rejecting the null, that this group has a sex ratio that is statistically significant (at the 95% confidence level) that they are away from the norm.  This could just be normal variability.  Running studies with larger sample size would narrow the CI range, and potentially then show whether the ratio truly diverge from the norm with enough statistic rigors._

\newpage

### Submission

For assignments in this class, you'll be submitting using the **Terminal** tab in the pane below. In order for the submission to work properly, make sure that:

1. Any image files you add that are needed to knit the file are in the `src` folder and file paths are specified accordingly.
2. You **have not changed the file name** of the assignment.
3. The file knits properly.

Once you have checked these items, you can proceed to submit your assignment.

1. Click on the **Terminal** tab in the pane below.
2. Copy-paste the following line of code into the terminal and press enter.

cd; cd phw142-sp22/lab/lab10; python3 turn_in.py

3. Follow the prompts to enter your Gradescope username and password.
4. If the submission is successful, you should see "Submission successful!" appear as the output. **Check your submission on the Gradescope website to ensure that the autograder worked properly and you received credit for your correct answers. If you think the autograder is incorrectly grading your work, please post on piazza!**
5. If the submission fails, try to diagnose the issue using the error messages--if you have problems, post on Piazza under the post "Datahub Issues".

The late policy will be strictly enforced, **no matter the reason**, including submission issues, so be sure to submit early enough to have time to diagnose issues if problems arise.
