---
title: "Lab07:  Central limit theorem and inference"
author: "Tin Ho 3033518349"
date: "2022-03-26"
output: pdf_document
---



### Instructions 
* Due date: Monday, March 21 at 11:59pm PST 
* Late penalty: 50% late penalty if submitted within 24 hours of due date, no marks for assignments submitted thereafter.
Remember: This homework does not involve autograder checking!

The `\newpage` tags allow your submission to upload with the answers in the correct locations for grading.  
* Submission process: Once finished, export your PDF and upload directly to Gradescope. 
**Make sure to not remove or add any `\newpage` tags**

Helpful hints:

- Every function you need to use was taught during lecture! So you may need to 
revisit the lecture code to help you along by opening the relevant files on Datahub. 
Alternatively, you may wish to view the code in the condensed PDFs posted 
on the course website. Good luck!

- Knit your file early and often to minimize knitting errors! If you copy and 
paste code for the slides, you are bound to get an error that is hard to 
diagnose. Typing out the code is the way to smooth knitting! We recommend 
knitting your file each time after you write a few sentences/add a new code 
chunk, so you can detect the source of knitting errors more easily. This will 
save you and the GSIs from frustration! You must knit correctly before submitting.*

- If your code runs off the page of the knitted PDF then you will LOSE POINTS! To
avoid this, have a look at your knitted PDF and ensure all the code fits in the 
file (you can easily view it on Gradescope via the provided link after submitting). 
If it doesn't look right, go back to your .Rmd file and add spaces (new lines) using 
the return or enter key so that the code runs onto the next line.

- Useful mathematical notation in markdown:

$$\mu$$

$$\sigma$$
\newpage



## Overview: 

In today's lab you will use some online resources, read some texts and watch some videos about the central limit theorem and about differences between p values and confidence intervals. You will also review information about p-hacking and data dredging to bring you up-to-speed on some of the language used to talk about bad scientific practice around the misuse of p-values.

### Section 1:  Central Limit Theorem

Open this link https://seeing-theory.brown.edu/probability-distributions/index.html#section3 and click on "Central Limit Theorem".

What you see is a simulation of a sampling process.  Set the $\alpha$ value to 5.00 and the $\beta$ value to 5.00  Notice the shape of the distribution (in yellow) at the top of the right hand side of the screen. 

#### Question 1a
What is the shape of this distribution? 


_Normal._


\newpage

Now set the sample size to 4 and the draws to 50 and click "sample":  you should see a simulation of 10 samples of 5 observations being drawn.  The distribution at the bottom now shows the distribution of means that were calculated in the random samples.  

Now try again, setting the sample size to 10 and the draws to 50.


#### Question 1b

Now set the parameters as follows:  $\alpha=0.6$ $\beta=1$ sample size = 4 draws = 50.

Then set the parameters to:  $\alpha=0.6$ $\beta=1$ sample size = 15 draws = 50. 

How does the distribution of sample means change when the sample size increases?

_The shape of the bell curve got narrower as sample size increased.  ie variance reduced _

Play with the parameters a bit to change the shape of the original distribution and the sample size - notice when the histogram looks normally distributed and when it does not.

\newpage

### Section 2:

To do: 

- Watch this 11-min Youtube video on P-hacking: https://www.youtube.com/watch?v=Gx0fAjNHb1M

- Read this Wikipedia article on data dredging: https://en.wikipedia.org/wiki/Data_dredging

- Read this Vox article about the Cornell food researcher: https://www.vox.com/science-and-health/2018/9/19/17879102/brian-wansink-cornell-food-brand-lab-retractions-jama

- Read this two-page ASA brief on statistical significance and p-values: https://www.amstat.org/asa/files/pdfs/P-ValueStatement.pdf

In your own words: 

#### Question 2a

What is p-hacking?

_p-hacking is the most important lesson out of PHW-142! p-hacking boils down to looking and look again until you find what you are looking for.  It is like flipping coins enough time until we get 3 heads in a row.  due to random nature, we flip the coins enough time, eventually the less likely event will show up.  So the food and environment research done by the cornell vilain is just asking his students to look and look and look some more.  with a cut off value for alpha at 0.05, getting 20 students to look at different combination of conditions will result be more likely than not of stumbling into one experiment that will have a stat significant result.  who said there were lies, damn lies, and statistics? :-P_

\newpage

#### Question 2b

What is data dredging?

_errr... I thought data dreding was the same as p-hacking, maybe a term that was more popular before computer popularized "hacking"?  Wikipedia list them as synomyms in the first line._

\newpage

#### Question 2c

One of these sources provides an example of p-hacking in epidemiology related to cancer clusters. Describe in your own words what the problem is.

_overweight is a huge problem, in America, developed countries and increasingly even in less developed countries, where the labor force tends to have more physical work than the sedentary office work of MDC.  overeating, consuming too many calories, is one avenue that leads to overweight.  Wansink popularized a theory that the environment and behavior can help reduce portion, pick healthier food choice, and overall reduce overeating and therefore overweight.  The theory has appeal, but theory is just a thought until there are things to prove it, and Wansink used data to prove his points.  Except that it seems that, while he did not fabricate the data, he understood the theory of p-value and knew that he could hack it to work to his benefits, to help prove his points. He understood that getting a p-value < 0.05 is not hard when the "experiments" is repeated many times, eg > 20 times, then the chance of finding something significant will turn out to be more likely than not, cuz the math on p-values when repeated will just lead to the equivalent of a much more linean cut off value.   Instead of repeating the same study enough till data show significance, he used slightly different scenaries.  but when so many scenaries were looked at, but all kinda boils down to just behavior or env that varies like 1 degree out of 360, some studies eventually showed significance, and he cherry picked such studies to "prove his points", and locking the non significant studies into vault that should never see the light of day._

\newpage

#### Question 2d

What are three practices noted in one of the articles to reduce p-hacking? Name each one and describe them in 1-2 sentences.

_Decide ahead of time how to analyze the data and what is really statistically significant, before carrying out the data analysis.  register what studies will be done before carrying them out, so that one cannot cherry pick studies that show stat significance.  If a barrage of studies will be done, then to avoid p-hacking, the cut off value need to be adjusted accordingly eg alpha/number of planned studies to indicate significance.  finally, p-value is just some stat significance.  correlation is not causation!  Have an open and critical mind in evaluating the data and see if the conclusion is indeed sound, don't boil reasearch down to a single number called p-value._

\newpage

#### Question 2e

One of the sources give a correction method for calculating p-value when you are going to conduct multiple tests. What is the name of the method? Write down the correction using an equation.

_Adjust the Family wise error rate using the Bonferroni correction, choose a new alpha, 0.05/n, where n is number of studies to be performed.  eg 0.05/5 = 0.01, and use this as the new cut off for significance, so that all the studies combined will only have a Type 1 error rate of 0.05_

\newpage

### Section 3:

Please watch the video here:

https://www.youtube.com/watch?reload=9&v=5OL1RqHrZQ8

With captions: https://youtu.be/hes_5Xds8_U


#### Question 3a

Which p-value is mentioned as leading to “Elation”?

_err... my take away from the video is that the p-value is next to useless.  Geoff Cumming himself said not to use it.  ._

\newpage

#### Question 3b

How big was the “true” difference in the imaginary experiment described?

_the p-value "dances" around, from non significant, to slightly significant, to even highly significant!  The distribution of the p-values is all over the place, and the highly significant one actually expected to happens in about 10% of the time (this in the backdrop of p-value > .10 which is not significant, at 36%) .  Overall, Cumming says p value tell us almost nothing! ._

\newpage

#### Question 3c

Which measure gave a better estimate of the variability in results over multiple simulated studies?

_The CI!_

#### Remember: download your knitted PDF and upload directly to Gradescope - the final document should be 11 pages long.


